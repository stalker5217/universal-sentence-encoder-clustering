{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Clustering  \n",
    "\n",
    "문장으로 구성된 Raw Data에서 의미가 비슷한 것들을 클러스터로 묶어 분류하는 작업을 진행할 예정이다.\n",
    "이 작업이 어떤 과정으로 이루어지는 필요한 개념과 도구를 알아본다.\n",
    "\n",
    "러프하게 보면 각 문장들을 수치화하고 이 수치를 기반으로, 유사한 값들을 가진 것들끼리 묶는 과정으로 진행된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet 가져오기  \n",
    "\n",
    "Dataset은 Google BigQuery Public Data Set을 이용한다.\n",
    "\n",
    "> [Google Cloud Public Datasets](https://cloud.google.com/public-datasets/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.colab import auth\n",
    "\n",
    "auth.authenticate_user()\n",
    "\n",
    "project_id = 'bigquery-public-data'\n",
    "\n",
    "# # Create a \"Client\" object\n",
    "client = bigquery.Client(project=project_id)\n",
    "\n",
    "# Construct a reference to the \"github_repos\" dataset\n",
    "dataset_ref = client.dataset(\"github_repos\")\n",
    "\n",
    "# API request - fetch the dataset\n",
    "dataset = client.get_dataset(dataset_ref)\n",
    "\n",
    "# Construct a reference to the \"commits\" table\n",
    "table_ref = dataset_ref.table(\"commits\")\n",
    "\n",
    "# API request - fetch the table\n",
    "table = client.get_table(table_ref)\n",
    "\n",
    "# Extract commit message, convert to dataframe\n",
    "dataframe = client.list_rows(table, \n",
    "                 selected_fields=[\n",
    "                      bigquery.SchemaField(\"subject\", \"STRING\")            \n",
    "                 ],\n",
    "                 max_results=1000).to_dataframe()\n",
    "\n",
    "commitMessages = dataframe['subject']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding  \n",
    "\n",
    "Embedding의 개념은 표현하고자 하는 대상을 dense vector 형태로 변환하는 것을 말한다. \n",
    "그 대상은 단어, 문장, 문서 전체, 이미지 등이 될 수 있다.  \n",
    "\n",
    "여기서는 문장을 임베딩하는 Universal Sentence Encoder를 사용한다. \n",
    "입력으로 텍스트가 주어지고 출력으로는 512차원의 vector가 생성된다. \n",
    "google에서 만들었으며 tf_hub에 공개되어있다.\n",
    "\n",
    "> [Universal Sentence Encoder](https://arxiv.org/abs/1803.11175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow_text\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow_text import SentencepieceTokenizer\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")\n",
    "embeddings = embed(commitMessages)\n",
    "\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering  \n",
    "\n",
    "수치화된 문장 즉, 임베딩 결과를 가지고 K-Means로 클러스터링한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_CNT = 10\n",
    "\n",
    "kmeans = KMeans(n_clusters = CLUSTER_CNT)\n",
    "kmeans.fit(embeddings)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "labelDict = dict()\n",
    "\n",
    "# key : cluster number\n",
    "# value : commit message array\n",
    "for i, commitMessage in enumerate(commitMessages):\n",
    "  if labels[i] in labelDict.keys():\n",
    "    labelDict[labels[i]].append(commitMessage)\n",
    "  else:\n",
    "    labelDict[labels[i]] = [commitMessage]\n",
    "\n",
    "for label in labelDict.keys():\n",
    "  for commitMessage in labelDict[label]:\n",
    "    print('{} : {}'.format(label, commitMessage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elbow Method  \n",
    "\n",
    "K-Means의 한계점으로는 클러스터 개수 선정에 있다.  \n",
    "위의 예제에서는 10개의 클러스터로 분류를 하도록 진행하였는데, \n",
    "적절한 클러스터 개수를 어느정도 파악할 수 있는 elbow method을 적용해본다.  \n",
    "\n",
    "그래프가 많이 꺾이면서 이 후 부터는 값의 변화가 거의 없는 지점을 elbow라고 한다. \n",
    "이 근처의 값을 선택하는 것이 어느정도 최적화 된 클러스터의 수라고 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sse = []\n",
    "for cluster_cnt in range(1, 20):\n",
    "  kmeans = KMeans(n_clusters=cluster_cnt, init='k-means++', random_state=0)\n",
    "  kmeans.fit(embeddings)\n",
    "  sse.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1, 20), sse, marker='o')\n",
    "plt.xlabel('cluster cnt')\n",
    "plt.ylabel('SSE')\n",
    "plt.show()"
   ]
  }
 ]
}